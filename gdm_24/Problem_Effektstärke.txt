Mögliche Ansätze wegen der geringen Effektstärke:

- Plausible Values ziehen 
- Itemschwierigkeiten mit doppelter Personenzahl ("echte" virtuelle Personen)
- Itemschwierigkeiten mit Items aller Gruppen (und nicht nur 1,2 oder 1,3)
- Nicht tam.person nehmen für Personenparameter
- Schritt für Schritt nochmal die Erzeugung der Daten überprüfen
- Andere Plausibilitätsprüfungen, z.B. Lösungsraten der Items
- Ist das gesamte Raschmodell überhaupt gültig??


Probleme auf Testebene: 
- Einzelne Items unterscheiden nicht zwischen guten und schlechten SuS (z.B. MC Items mit falschen Lösungen (genau ein vs mind. ein Schnittpunkt) )
- Willkürlich festgesetzte Grenzen für einen  Punkt bei MC Items 
- Einzelne Items gehören nicht zur prozeduralen bzw. zur konzeptuellen Skala (z.B. Item zur Lösung des LGS mit keiner Lösung / unendlich vielen LÖsungen erfordert definitiv auch konzeptuelles Wissen)
			-> Gültigkeit meiner Dimensionen prüfen (wie bei klassisdcher Testtheorie konfirmatorische Faktorenanalyse): gibt es einen Gültigkeitsparameter ob mein gesamtes Raschodell passt / Sinn machen und somit auch meine Dimensionen sinnvoll sind?



BEobachtungen bei den Daten:
Aufgabe a1_6: Felix kodiert meistens ku, was zu einer 0 führt, fast nie ks, was zu einer 1 führt (gleiches Problem bei b1_6)
Aufgabe b2_5 (evtl. betrifft es gar nicht die Aufgabe sondern das Pendant inn b2_5): Theoretisch gibt es zwei Zeilen in denen ein Fehler vorliegt!

Aus irgendwelchen Gründen fehlt im kodierten Datensatz von Felix die erste Zeile: Wieso?

In der Funktion evaluate_mc_tasks wurden na Werte einfach mit 0 kodiert. Ist das sinnvoll?